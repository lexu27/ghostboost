The movie "Coded Bias" tells a lot about problems with face-finding computer programs. It talks about how these programs might not work well for all people because the people who made them focused more on white faces. This means they didn't have enough information about people with darker skin or that most of the people who made the programs were biased. This makes it hard to treat everyone fairly when it comes to race and gender, and it seems like a big problem because the programs might not meet everyone's needs. Also, not knowing how the programs use data and decide things doesn't help to get rid of these problems.

To make these programs better and fairer, some suggestions are not using them in legal stuff and police work and making them better. The first idea is about taking responsibility for choices, and the second idea has many parts and should start with updating the data used and having other people check if the programs work well. By doing these things, the bias about race and gender can get better with time, and the programs can work well.

These ideas are really important for using technology in the right way, and they can improve how well the programs work and solve the problem of watching people and treating some groups unfairly. This idea is based on the fact that people should take responsibility for choices clearly. This change can happen if the programs do what they're supposed to do, and getting rid of bias can happen if people check if the programs work well inside and outside. So, the future of this field and how useful it is for investigations depends on how experts work to solve problems and fix what Nakamura and Browne are worried about.