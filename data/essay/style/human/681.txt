The story by Will Douglas Heaven called “The new version of GPT-3 has much better behaved (and should be less toxic)” talks about how a big change happened to Generative Pre-Trained Transformer 3 or GPT-3 because of the InstructGPT update. It says that “big language machines like GPT-3 are trained using lots of text, much from the internet, where they see the good and bad things people write” (Heaven par. 2). Because this tool from OpenAI learns from all kinds of English texts, it used to make a lot of mean and bad words. But the new change makes this problem much smaller, so it makes fewer wrong, wrong, and mean texts.

It is good to remember that GPT-3 is still new, which means its effects are small now. But the changes, like InstructGPT, help AI learn faster, making it better as a tool. The future business effects are very big, where jobs that repeat and do not need creativity will not be needed because GPT-3 can do these tasks better and faster.

In ending, I see GPT-3 as a very good tool that needs a lot of teaching often. I think GPT-3 should not only learn by itself, and experts should guide it. This text maker is very important for making a big change in moving towards using computers for many text jobs, like coding. Even though it will not replace all coders, it will make the boring parts of making code very good.