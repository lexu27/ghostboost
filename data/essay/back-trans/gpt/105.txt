In the documentary "Coded Vor Bed" staged by Shalini Kantayya, the main problems in connection with facial recognition algorithms are brought to light. This algorithms suffer with the intention to improve security and efficiency, prejudices that influence racist minorities disproportionately and immortalize social inequalities.In this essay, the most important problems that are discussed in the film and how on breed and gender based on breed and gender can be reduced in these algorithms.examined the monitoring and investigations in which racist minorities are involved.
One of the fundamental problems in the facial recognition algorithm, which is highlighted in the film, is their racist and gender prejudices. The training data of the algorithms, which mainly consist of white male faces, leads to biased results that false individuals from racist minority groups, especially women, and reflects this distortationA direct reflection of social prejudices that have been accidentally integrated into the algorithms. This problem is the first step towards the solution.
In order to reduce the distortion based on the breed and gender, diverse and representative data records are of crucial importance. It is important to have extensive databases that contain a wide range of ethnicia, sexes and other different groups.Check systems regularly for distortions and make the necessary adjustments. A diverse team of programmers who can offer different perspectives and knowledge can also contribute to fairer algorithms.
The accountability obligation plays an important role in ensuring the appropriate use of technology in monitoring and examinations in which racist minorities are involved. As can be seen in the film, facial recognition technology was used to target certain communities, which leads to illegal arrests and increasedRace profiles led to counteract such abuse, strict regulations and supervisory mechanisms should be implemented. Unsigned audits and transparent reporting should be mandatory for organizations that use facial recognition technology.against marginalized communities.
In addition, including marginalized communities in decision -making processes, the entry of people who are the most affected can be interpreted in such a way that they protect against distortions and potential damage.are, the ethical effects of facial recognition technology and their effects on marginalized groups should take into account.
In summary, it can be said that the "coded bias" illuminates the main problems in connection with facial recognition algorithms, in particular their racist and gender -specific prejudices, and to reduce these prejudices, it is important to develop different data records, to check regularly, algorithms and different teamsInclude in their creation. In this way, responsibility measures such as independent audits, transparent reporting and legal consequences for the abuse of misuse are of crucial importance in order to ensure the proper use of technology in monitoring and investigations with racist minorities.To create fairer and fairer society that respects the rights and dignity of all people.