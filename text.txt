### 1. Why did the authors choose to address this topic?Â 
* The authors chose to address this topic because of the massive need for large labeled datasets in machine learning applications. The authors note that collecting data is often the most expensive part of applying machine learning algorithms. They note that while hand-labeled features are often very helpful when creating accurate machine learning applications, they just aren't plentiful enough. To deal with this issue, the authors introduce the data programming paradigm.
### 2. What is the research context of this work? How did the community think about this topic before this paper?
* Prior to this work, the research community has attempted to programatically create training sets. For instance, Distant supervision leverages external knowledge bases to label data by typically mapping relations onto text. However, since the mappings are typically just heuristics, the labels are typically very noisy. Crowdsourcing has also been quite popular in labelling data since we have a large set of labelers. The benefit of crowdsourcing too is that there's not direct modeling dependency between the labelers. Other works have explored co-training, which we have covered in a previous paper in class already, and boosting, where multiple weak classifiers are combined together to create a stronger classifier.
* Most of these past works try to go directly from the data to a label, while the data programming framework emphasizes "programs" for the data labelling process.
### 3. What is the contribution of this paper? How is the topic better addressed after this paper?
* This paper introduces a new paradigm for creating supervised datasets called data programming. Instead of having labelers painstakingly go through their data and generate labels, they model the data labelling process in terms of *labeling functions (LFs)* which are user-created programs that leverage heuristics to label data. Since these LFs can be noisy, data programming essentially trains a model to learn the accuracies and correlations between the LFs to "denoise" the generated training set. They authors also introduce a dependency graph between the LFs to further improve accuracy as the assumption that the user defined LFs are all independent is faulty.
* This new paradigm allows researchers and companies to spend less time manually labelling the data and focus more on employing domain experts to write label functions, which could be easier.
### 4. What could go wrong if dependencies between labeling functions exist, but are not modelled/handled? You might consider comparing two settings: (setting 1) one high accuracy labeling function and many low accuracy labeling functions where there are no dependencies, and (setting 2) one high accuracy labeling function and many low accuracy labeling functions where there are correlation dependencies between the low accuracy labeling functions.
* If the dependencies between labelling functions aren't explicitly modeled, it's entirely possible that the generative model responsible for creating the training set becomes overly biased. For instance in setting two, if we treat the low accuracy LFs as independent, our model can drastically overestimate the confidence of their labels. To see this, imagine that our low accuracy LFs all label a certain datapoint with high certainty. The data programming model will see this as a high indication of correctness when in reality, these LFs are just making the same types of errors together. 
